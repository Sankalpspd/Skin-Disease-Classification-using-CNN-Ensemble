{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ_u-bK7Cv6K",
        "outputId": "028b222c-ac4d-4bdd-90be-d20a343f30bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Skin_Disease_Dataset.zip\" /content/"
      ],
      "metadata": {
        "id": "Ck2ameunC8Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/Skin_Disease_Dataset.zip\" -d /content/skin_dataset"
      ],
      "metadata": {
        "id": "oip_fGVXDARO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/skin_dataset/skin Disease Dataset/kaggle\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvG-llXCFWFj",
        "outputId": "d8edabf5-072a-4861-f03c-ec6453afc228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  train  val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/skin_dataset/skin Disease Dataset/kaggle/train\"\n",
        "val_path   = \"/content/skin_dataset/skin Disease Dataset/kaggle/val\"\n",
        "test_path  = \"/content/skin_dataset/skin Disease Dataset/kaggle/test\""
      ],
      "metadata": {
        "id": "30LGnsQdDGpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "wyLe3nr9DMwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/models"
      ],
      "metadata": {
        "id": "7b2PdmTpDQs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "\n",
        "NUM_CLASSES = 6\n",
        "SAMPLES_PER_CLASS = 3000\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\"/content/skin_dataset/skin Disease Dataset/kaggle/train\")\n",
        "targets = np.array(train_dataset.targets)\n",
        "\n",
        "balanced_dir = \"/content/skin_dataset/skin Disease Dataset/kaggle/train_balanced\"\n",
        "os.makedirs(balanced_dir, exist_ok=True)\n",
        "\n",
        "if os.path.exists(balanced_dir):\n",
        "    shutil.rmtree(balanced_dir)\n",
        "\n",
        "os.makedirs(balanced_dir)\n",
        "\n",
        "for cls in range(NUM_CLASSES):\n",
        "    cls_indices = np.where(targets == cls)[0]\n",
        "    selected = np.random.choice(cls_indices, SAMPLES_PER_CLASS, replace=len(cls_indices)<SAMPLES_PER_CLASS)\n",
        "\n",
        "    class_dir = os.path.join(balanced_dir, train_dataset.classes[cls])\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    for j, idx in enumerate(selected):\n",
        "     src_path = train_dataset.imgs[idx][0]\n",
        "\n",
        "     filename = f\"{j}_{os.path.basename(src_path)}\"\n",
        "     dst_path = os.path.join(class_dir, filename)\n",
        "\n",
        "     shutil.copy(src_path, dst_path)\n"
      ],
      "metadata": {
        "id": "Sx2zjOhAHg-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/skin_dataset/skin Disease Dataset/kaggle\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxfUE3cnHzlh",
        "outputId": "654b0b1f-dbe2-47c2-dacf-a1e4b83ab491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  train  train_balanced  val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
        "\n",
        "weights = EfficientNet_B1_Weights.IMAGENET1K_V1\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((240, 240)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((240, 240)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "metadata": {
        "id": "fYajXqEdDb7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(\"/content/skin_dataset/skin Disease Dataset/kaggle/train_balanced\", transform=train_transforms)\n",
        "val_dataset   = datasets.ImageFolder(\"/content/skin_dataset/skin Disease Dataset/kaggle/val\", transform=val_transforms)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "Qmk78ucVDfrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVdEplDEIPIE",
        "outputId": "c099c514-b9e3-4ec0-85d0-83bd325afbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
        "\n",
        "weights = EfficientNet_B1_Weights.IMAGENET1K_V1\n",
        "model_en = efficientnet_b1(weights=weights)\n",
        "\n",
        "for param in model_en.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_en.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(model_en.classifier[1].in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, NUM_CLASSES)\n",
        ")\n",
        "\n",
        "model_en = model_en.to(DEVICE)"
      ],
      "metadata": {
        "id": "J0LAftrfDgxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43534a51-1f5e-4ef5-a103-758b90ce3b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30.1M/30.1M [00:00<00:00, 197MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.00001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_en.classifier.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "NVHgNW0BDkPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 64\n",
        "PATIENCE = 5\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "best_val_f1 = 0\n",
        "MODEL_PATH = \"/content/drive/MyDrive/models/efficientnet_b1_1.pth\"\n",
        "\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    model_en.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_en(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        preds_np = preds.detach().cpu().numpy()\n",
        "        labels_np = labels.detach().cpu().numpy()\n",
        "\n",
        "        all_train_preds.extend(preds_np)\n",
        "        all_train_labels.extend(labels_np)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        batch_acc = (preds_np == labels_np).mean()\n",
        "        batch_precision = precision_score(labels_np, preds_np, average='macro', zero_division=0)\n",
        "        batch_recall = recall_score(labels_np, preds_np, average='macro', zero_division=0)\n",
        "        batch_f1 = f1_score(labels_np, preds_np, average='macro', zero_division=0)\n",
        "\n",
        "        sys.stdout.write(\n",
        "            f\"\\rEpoch {epoch+1}/{EPOCHS} | \"\n",
        "            f\"Batch {i}/{len(train_loader)} | \"\n",
        "            f\"Loss: {loss.item():.4f} | \"\n",
        "            f\"Acc: {batch_acc:.4f} | \"\n",
        "            f\"Prec: {batch_precision:.4f} | \"\n",
        "            f\"Rec: {batch_recall:.4f} | \"\n",
        "            f\"F1: {batch_f1:.4f}\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print()\n",
        "\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "    train_acc = np.mean(np.array(all_train_preds) == np.array(all_train_labels))\n",
        "    train_precision = precision_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "    train_recall = recall_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "\n",
        "    model_en.eval()\n",
        "\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model_en(inputs)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = np.mean(np.array(val_preds) == np.array(val_labels))\n",
        "    val_precision = precision_score(val_labels, val_preds, average='macro', zero_division=0)\n",
        "    val_recall = recall_score(val_labels, val_preds, average='macro', zero_division=0)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary\")\n",
        "    print(f\"Train | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Prec: {train_precision:.4f} | Rec: {train_recall:.4f} | F1: {train_f1:.4f}\")\n",
        "    print(f\"Val   | Acc: {val_acc:.4f} | Prec: {val_precision:.4f} | Rec: {val_recall:.4f} | F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model_en.state_dict(), MODEL_PATH)\n",
        "        counter = 0\n",
        "        print(\"Model improved. Saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement. Patience: {counter}/{PATIENCE}\")\n",
        "\n",
        "        if counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgxtfRfvDwtS",
        "outputId": "c66a4d2b-363b-4271-f718-4b2acf06dc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 | Batch 282/282 | Loss: 1.6259 | Acc: 0.5000 | Prec: 0.3000 | Rec: 0.3139 | F1: 0.3000\n",
            "\n",
            "Epoch 1 Summary\n",
            "Train | Loss: 1.7167 | Acc: 0.3386 | Prec: 0.3444 | Rec: 0.3386 | F1: 0.3150\n",
            "Val   | Acc: 0.5195 | Prec: 0.4757 | Rec: 0.5232 | F1: 0.4744\n",
            "✔ Model improved. Saved.\n",
            "Epoch 2/25 | Batch 282/282 | Loss: 1.3428 | Acc: 0.6875 | Prec: 0.5643 | Rec: 0.5767 | F1: 0.5633\n",
            "\n",
            "Epoch 2 Summary\n",
            "Train | Loss: 1.5506 | Acc: 0.4951 | Prec: 0.5162 | Rec: 0.4951 | F1: 0.4833\n",
            "Val   | Acc: 0.5682 | Prec: 0.5144 | Rec: 0.5706 | F1: 0.5283\n",
            "✔ Model improved. Saved.\n",
            "Epoch 3/25 | Batch 282/282 | Loss: 1.4743 | Acc: 0.3750 | Prec: 0.2472 | Rec: 0.3333 | F1: 0.2836\n",
            "\n",
            "Epoch 3 Summary\n",
            "Train | Loss: 1.3983 | Acc: 0.5439 | Prec: 0.5504 | Rec: 0.5439 | F1: 0.5374\n",
            "Val   | Acc: 0.5763 | Prec: 0.5225 | Rec: 0.5766 | F1: 0.5384\n",
            "✔ Model improved. Saved.\n",
            "Epoch 4/25 | Batch 282/282 | Loss: 0.9757 | Acc: 0.7500 | Prec: 0.6667 | Rec: 0.6889 | F1: 0.6593\n",
            "\n",
            "Epoch 4 Summary\n",
            "Train | Loss: 1.2895 | Acc: 0.5581 | Prec: 0.5594 | Rec: 0.5581 | F1: 0.5540\n",
            "Val   | Acc: 0.5794 | Prec: 0.5238 | Rec: 0.5835 | F1: 0.5409\n",
            "✔ Model improved. Saved.\n",
            "Epoch 5/25 | Batch 282/282 | Loss: 1.1926 | Acc: 0.5625 | Prec: 0.6556 | Rec: 0.6389 | F1: 0.5889\n",
            "\n",
            "Epoch 5 Summary\n",
            "Train | Loss: 1.2192 | Acc: 0.5751 | Prec: 0.5740 | Rec: 0.5751 | F1: 0.5724\n",
            "Val   | Acc: 0.5753 | Prec: 0.5233 | Rec: 0.5795 | F1: 0.5386\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 6/25 | Batch 282/282 | Loss: 1.1147 | Acc: 0.5625 | Prec: 0.6667 | Rec: 0.4722 | F1: 0.5230\n",
            "\n",
            "Epoch 6 Summary\n",
            "Train | Loss: 1.1674 | Acc: 0.5846 | Prec: 0.5832 | Rec: 0.5846 | F1: 0.5826\n",
            "Val   | Acc: 0.5850 | Prec: 0.5331 | Rec: 0.5890 | F1: 0.5495\n",
            "✔ Model improved. Saved.\n",
            "Epoch 7/25 | Batch 282/282 | Loss: 0.9784 | Acc: 0.6875 | Prec: 0.6806 | Rec: 0.8167 | F1: 0.6687\n",
            "\n",
            "Epoch 7 Summary\n",
            "Train | Loss: 1.1381 | Acc: 0.5891 | Prec: 0.5862 | Rec: 0.5891 | F1: 0.5865\n",
            "Val   | Acc: 0.5955 | Prec: 0.5441 | Rec: 0.5970 | F1: 0.5612\n",
            "✔ Model improved. Saved.\n",
            "Epoch 8/25 | Batch 282/282 | Loss: 0.8855 | Acc: 0.6875 | Prec: 0.6528 | Rec: 0.5833 | F1: 0.6012\n",
            "\n",
            "Epoch 8 Summary\n",
            "Train | Loss: 1.1088 | Acc: 0.5961 | Prec: 0.5948 | Rec: 0.5961 | F1: 0.5947\n",
            "Val   | Acc: 0.5975 | Prec: 0.5440 | Rec: 0.6109 | F1: 0.5633\n",
            "✔ Model improved. Saved.\n",
            "Epoch 9/25 | Batch 282/282 | Loss: 1.2576 | Acc: 0.3750 | Prec: 0.4778 | Rec: 0.3611 | F1: 0.3476\n",
            "\n",
            "Epoch 9 Summary\n",
            "Train | Loss: 1.0886 | Acc: 0.6002 | Prec: 0.5979 | Rec: 0.6002 | F1: 0.5985\n",
            "Val   | Acc: 0.5965 | Prec: 0.5438 | Rec: 0.6123 | F1: 0.5615\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 10/25 | Batch 282/282 | Loss: 1.6577 | Acc: 0.5625 | Prec: 0.5833 | Rec: 0.6667 | F1: 0.5556\n",
            "\n",
            "Epoch 10 Summary\n",
            "Train | Loss: 1.0665 | Acc: 0.6083 | Prec: 0.6058 | Rec: 0.6083 | F1: 0.6063\n",
            "Val   | Acc: 0.6074 | Prec: 0.5537 | Rec: 0.6168 | F1: 0.5716\n",
            "✔ Model improved. Saved.\n",
            "Epoch 11/25 | Batch 282/282 | Loss: 1.8308 | Acc: 0.4375 | Prec: 0.4861 | Rec: 0.3472 | F1: 0.3935\n",
            "\n",
            "Epoch 11 Summary\n",
            "Train | Loss: 1.0539 | Acc: 0.6116 | Prec: 0.6108 | Rec: 0.6116 | F1: 0.6107\n",
            "Val   | Acc: 0.6115 | Prec: 0.5602 | Rec: 0.6279 | F1: 0.5797\n",
            "✔ Model improved. Saved.\n",
            "Epoch 12/25 | Batch 282/282 | Loss: 0.9555 | Acc: 0.7500 | Prec: 0.7778 | Rec: 0.7778 | F1: 0.7365\n",
            "\n",
            "Epoch 12 Summary\n",
            "Train | Loss: 1.0429 | Acc: 0.6169 | Prec: 0.6151 | Rec: 0.6169 | F1: 0.6154\n",
            "Val   | Acc: 0.6136 | Prec: 0.5611 | Rec: 0.6231 | F1: 0.5788\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 13/25 | Batch 282/282 | Loss: 0.9690 | Acc: 0.6250 | Prec: 0.6389 | Rec: 0.6278 | F1: 0.6194\n",
            "\n",
            "Epoch 13 Summary\n",
            "Train | Loss: 1.0295 | Acc: 0.6221 | Prec: 0.6212 | Rec: 0.6221 | F1: 0.6209\n",
            "Val   | Acc: 0.6197 | Prec: 0.5683 | Rec: 0.6332 | F1: 0.5869\n",
            "✔ Model improved. Saved.\n",
            "Epoch 14/25 | Batch 282/282 | Loss: 1.0002 | Acc: 0.6875 | Prec: 0.5417 | Rec: 0.7028 | F1: 0.5906\n",
            "\n",
            "Epoch 14 Summary\n",
            "Train | Loss: 1.0194 | Acc: 0.6272 | Prec: 0.6257 | Rec: 0.6272 | F1: 0.6260\n",
            "Val   | Acc: 0.6210 | Prec: 0.5636 | Rec: 0.6314 | F1: 0.5818\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 15/25 | Batch 282/282 | Loss: 1.0683 | Acc: 0.7500 | Prec: 0.6200 | Rec: 0.6629 | F1: 0.6000\n",
            "\n",
            "Epoch 15 Summary\n",
            "Train | Loss: 1.0111 | Acc: 0.6282 | Prec: 0.6262 | Rec: 0.6282 | F1: 0.6267\n",
            "Val   | Acc: 0.6240 | Prec: 0.5686 | Rec: 0.6318 | F1: 0.5855\n",
            "No improvement. Patience: 2/5\n",
            "Epoch 16/25 | Batch 282/282 | Loss: 1.4449 | Acc: 0.3750 | Prec: 0.4306 | Rec: 0.4417 | F1: 0.3762\n",
            "\n",
            "Epoch 16 Summary\n",
            "Train | Loss: 1.0085 | Acc: 0.6257 | Prec: 0.6244 | Rec: 0.6257 | F1: 0.6245\n",
            "Val   | Acc: 0.6258 | Prec: 0.5725 | Rec: 0.6380 | F1: 0.5917\n",
            "✔ Model improved. Saved.\n",
            "Epoch 17/25 | Batch 282/282 | Loss: 0.8288 | Acc: 0.7500 | Prec: 0.7778 | Rec: 0.8889 | F1: 0.7722\n",
            "\n",
            "Epoch 17 Summary\n",
            "Train | Loss: 1.0061 | Acc: 0.6268 | Prec: 0.6253 | Rec: 0.6268 | F1: 0.6256\n",
            "Val   | Acc: 0.6324 | Prec: 0.5771 | Rec: 0.6389 | F1: 0.5959\n",
            "✔ Model improved. Saved.\n",
            "Epoch 18/25 | Batch 282/282 | Loss: 0.8505 | Acc: 0.7500 | Prec: 0.8194 | Rec: 0.7361 | F1: 0.7302\n",
            "\n",
            "Epoch 18 Summary\n",
            "Train | Loss: 0.9937 | Acc: 0.6327 | Prec: 0.6313 | Rec: 0.6327 | F1: 0.6316\n",
            "Val   | Acc: 0.6322 | Prec: 0.5802 | Rec: 0.6417 | F1: 0.5991\n",
            "✔ Model improved. Saved.\n",
            "Epoch 19/25 | Batch 282/282 | Loss: 0.8393 | Acc: 0.7500 | Prec: 0.8444 | Rec: 0.8583 | F1: 0.8000\n",
            "\n",
            "Epoch 19 Summary\n",
            "Train | Loss: 0.9869 | Acc: 0.6346 | Prec: 0.6341 | Rec: 0.6346 | F1: 0.6338\n",
            "Val   | Acc: 0.6398 | Prec: 0.5830 | Rec: 0.6493 | F1: 0.6012\n",
            "✔ Model improved. Saved.\n",
            "Epoch 20/25 | Batch 282/282 | Loss: 1.1114 | Acc: 0.5000 | Prec: 0.4722 | Rec: 0.4167 | F1: 0.4230\n",
            "\n",
            "Epoch 20 Summary\n",
            "Train | Loss: 0.9822 | Acc: 0.6375 | Prec: 0.6363 | Rec: 0.6375 | F1: 0.6364\n",
            "Val   | Acc: 0.6401 | Prec: 0.5834 | Rec: 0.6452 | F1: 0.6020\n",
            "✔ Model improved. Saved.\n",
            "Epoch 21/25 | Batch 282/282 | Loss: 1.1867 | Acc: 0.4375 | Prec: 0.4583 | Rec: 0.3333 | F1: 0.3583\n",
            "\n",
            "Epoch 21 Summary\n",
            "Train | Loss: 0.9769 | Acc: 0.6363 | Prec: 0.6353 | Rec: 0.6363 | F1: 0.6353\n",
            "Val   | Acc: 0.6424 | Prec: 0.5866 | Rec: 0.6446 | F1: 0.6056\n",
            "✔ Model improved. Saved.\n",
            "Epoch 22/25 | Batch 282/282 | Loss: 1.0773 | Acc: 0.6250 | Prec: 0.6667 | Rec: 0.5000 | F1: 0.5556\n",
            "\n",
            "Epoch 22 Summary\n",
            "Train | Loss: 0.9728 | Acc: 0.6394 | Prec: 0.6387 | Rec: 0.6394 | F1: 0.6387\n",
            "Val   | Acc: 0.6380 | Prec: 0.5820 | Rec: 0.6420 | F1: 0.6009\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 23/25 | Batch 282/282 | Loss: 1.1203 | Acc: 0.5625 | Prec: 0.5778 | Rec: 0.5000 | F1: 0.4942\n",
            "\n",
            "Epoch 23 Summary\n",
            "Train | Loss: 0.9622 | Acc: 0.6439 | Prec: 0.6429 | Rec: 0.6439 | F1: 0.6429\n",
            "Val   | Acc: 0.6406 | Prec: 0.5825 | Rec: 0.6460 | F1: 0.6016\n",
            "No improvement. Patience: 2/5\n",
            "Epoch 24/25 | Batch 282/282 | Loss: 0.8870 | Acc: 0.5625 | Prec: 0.5000 | Rec: 0.5833 | F1: 0.4991\n",
            "\n",
            "Epoch 24 Summary\n",
            "Train | Loss: 0.9671 | Acc: 0.6390 | Prec: 0.6378 | Rec: 0.6390 | F1: 0.6380\n",
            "Val   | Acc: 0.6467 | Prec: 0.5894 | Rec: 0.6542 | F1: 0.6083\n",
            "✔ Model improved. Saved.\n",
            "Epoch 25/25 | Batch 282/282 | Loss: 1.1899 | Acc: 0.4375 | Prec: 0.4722 | Rec: 0.5278 | F1: 0.4185\n",
            "\n",
            "Epoch 25 Summary\n",
            "Train | Loss: 0.9665 | Acc: 0.6439 | Prec: 0.6433 | Rec: 0.6439 | F1: 0.6432\n",
            "Val   | Acc: 0.6411 | Prec: 0.5835 | Rec: 0.6495 | F1: 0.6017\n",
            "No improvement. Patience: 1/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_en.features)"
      ],
      "metadata": {
        "id": "0rneO6NsQzaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4239fbb-9f16-4802-cc97-476b8f025ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2dNormActivation(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): SiLU(inplace=True)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
            "    )\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (0): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
            "    )\n",
            "    (2): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
            "    )\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (0): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
            "    )\n",
            "    (2): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
            "    )\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (0): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
            "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
            "    )\n",
            "    (2): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
            "    )\n",
            "    (3): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
            "    )\n",
            "  )\n",
            "  (5): Sequential(\n",
            "    (0): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
            "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
            "    )\n",
            "    (2): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
            "    )\n",
            "    (3): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
            "    )\n",
            "  )\n",
            "  (6): Sequential(\n",
            "    (0): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
            "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
            "    )\n",
            "    (2): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
            "    )\n",
            "    (3): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
            "    )\n",
            "    (4): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
            "    )\n",
            "  )\n",
            "  (7): Sequential(\n",
            "    (0): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
            "          (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): SiLU(inplace=True)\n",
            "          (scale_activation): Sigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
            "    )\n",
            "  )\n",
            "  (8): Conv2dNormActivation(\n",
            "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): SiLU(inplace=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning by unfreezing last 2 layers of the base model"
      ],
      "metadata": {
        "id": "bmPrNsGy2TQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_en.features.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_en.features[-2:].parameters():\n",
        "    param.requires_grad = True\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([\n",
        "    {\"params\": model_en.classifier.parameters(), \"lr\": 2e-5},\n",
        "    {\"params\": model_en.features[-2:].parameters(), \"lr\": 2e-6}\n",
        "], weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "nf7dZ8-XD3kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 64\n",
        "PATIENCE = 5\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "best_val_f1 = 0\n",
        "MODEL_PATH = \"/content/drive/MyDrive/models/efficientnet_b1_2.pth\"\n",
        "\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    model_en.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_en(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        preds_np = preds.detach().cpu().numpy()\n",
        "        labels_np = labels.detach().cpu().numpy()\n",
        "\n",
        "        all_train_preds.extend(preds_np)\n",
        "        all_train_labels.extend(labels_np)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        batch_acc = (preds_np == labels_np).mean()\n",
        "        batch_precision = precision_score(labels_np, preds_np, average='macro', zero_division=0)\n",
        "        batch_recall = recall_score(labels_np, preds_np, average='macro', zero_division=0)\n",
        "        batch_f1 = f1_score(labels_np, preds_np, average='macro', zero_division=0)\n",
        "\n",
        "        sys.stdout.write(\n",
        "            f\"\\rEpoch {epoch+1}/{EPOCHS} | \"\n",
        "            f\"Batch {i}/{len(train_loader)} | \"\n",
        "            f\"Loss: {loss.item():.4f} | \"\n",
        "            f\"Acc: {batch_acc:.4f} | \"\n",
        "            f\"Prec: {batch_precision:.4f} | \"\n",
        "            f\"Rec: {batch_recall:.4f} | \"\n",
        "            f\"F1: {batch_f1:.4f}\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print()\n",
        "\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "    train_acc = np.mean(np.array(all_train_preds) == np.array(all_train_labels))\n",
        "    train_precision = precision_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "    train_recall = recall_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "\n",
        "    model_en.eval()\n",
        "\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model_en(inputs)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = np.mean(np.array(val_preds) == np.array(val_labels))\n",
        "    val_precision = precision_score(val_labels, val_preds, average='macro', zero_division=0)\n",
        "    val_recall = recall_score(val_labels, val_preds, average='macro', zero_division=0)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary\")\n",
        "    print(f\"Train | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Prec: {train_precision:.4f} | Rec: {train_recall:.4f} | F1: {train_f1:.4f}\")\n",
        "    print(f\"Val   | Acc: {val_acc:.4f} | Prec: {val_precision:.4f} | Rec: {val_recall:.4f} | F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model_en.state_dict(), MODEL_PATH)\n",
        "        counter = 0\n",
        "        print(\"Model improved. Saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement. Patience: {counter}/{PATIENCE}\")\n",
        "\n",
        "        if counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpG3x9hJ4PO1",
        "outputId": "632d9244-29a1-4b21-80a2-64c3a57b9a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | Batch 282/282 | Loss: 0.8884 | Acc: 0.6250 | Prec: 0.6111 | Rec: 0.6667 | F1: 0.6063\n",
            "\n",
            "Epoch 1 Summary\n",
            "Train | Loss: 0.9534 | Acc: 0.6464 | Prec: 0.6459 | Rec: 0.6464 | F1: 0.6457\n",
            "Val   | Acc: 0.6482 | Prec: 0.5892 | Rec: 0.6542 | F1: 0.6078\n",
            "✔ Model improved. Saved.\n",
            "Epoch 2/15 | Batch 282/282 | Loss: 0.8239 | Acc: 0.6875 | Prec: 0.7639 | Rec: 0.7639 | F1: 0.6881\n",
            "\n",
            "Epoch 2 Summary\n",
            "Train | Loss: 0.9416 | Acc: 0.6477 | Prec: 0.6474 | Rec: 0.6477 | F1: 0.6470\n",
            "Val   | Acc: 0.6531 | Prec: 0.5959 | Rec: 0.6508 | F1: 0.6149\n",
            "✔ Model improved. Saved.\n",
            "Epoch 3/15 | Batch 282/282 | Loss: 1.2014 | Acc: 0.6250 | Prec: 0.4667 | Rec: 0.4944 | F1: 0.4778\n",
            "\n",
            "Epoch 3 Summary\n",
            "Train | Loss: 0.9242 | Acc: 0.6571 | Prec: 0.6564 | Rec: 0.6571 | F1: 0.6563\n",
            "Val   | Acc: 0.6513 | Prec: 0.5930 | Rec: 0.6598 | F1: 0.6129\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 4/15 | Batch 282/282 | Loss: 1.2737 | Acc: 0.4375 | Prec: 0.5694 | Rec: 0.4667 | F1: 0.4563\n",
            "\n",
            "Epoch 4 Summary\n",
            "Train | Loss: 0.9128 | Acc: 0.6583 | Prec: 0.6581 | Rec: 0.6583 | F1: 0.6577\n",
            "Val   | Acc: 0.6612 | Prec: 0.6031 | Rec: 0.6584 | F1: 0.6217\n",
            "✔ Model improved. Saved.\n",
            "Epoch 5/15 | Batch 282/282 | Loss: 1.1297 | Acc: 0.4375 | Prec: 0.4444 | Rec: 0.4306 | F1: 0.4286\n",
            "\n",
            "Epoch 5 Summary\n",
            "Train | Loss: 0.9033 | Acc: 0.6625 | Prec: 0.6625 | Rec: 0.6625 | F1: 0.6620\n",
            "Val   | Acc: 0.6587 | Prec: 0.6015 | Rec: 0.6659 | F1: 0.6216\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 6/15 | Batch 282/282 | Loss: 0.6858 | Acc: 0.7500 | Prec: 0.7639 | Rec: 0.7639 | F1: 0.7524\n",
            "\n",
            "Epoch 6 Summary\n",
            "Train | Loss: 0.8871 | Acc: 0.6674 | Prec: 0.6671 | Rec: 0.6674 | F1: 0.6668\n",
            "Val   | Acc: 0.6622 | Prec: 0.6048 | Rec: 0.6678 | F1: 0.6259\n",
            "✔ Model improved. Saved.\n",
            "Epoch 7/15 | Batch 282/282 | Loss: 1.0359 | Acc: 0.6250 | Prec: 0.5833 | Rec: 0.5833 | F1: 0.5437\n",
            "\n",
            "Epoch 7 Summary\n",
            "Train | Loss: 0.8799 | Acc: 0.6693 | Prec: 0.6701 | Rec: 0.6693 | F1: 0.6691\n",
            "Val   | Acc: 0.6625 | Prec: 0.6030 | Rec: 0.6656 | F1: 0.6224\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 8/15 | Batch 282/282 | Loss: 0.6848 | Acc: 0.6875 | Prec: 0.7500 | Rec: 0.7222 | F1: 0.7063\n",
            "\n",
            "Epoch 8 Summary\n",
            "Train | Loss: 0.8656 | Acc: 0.6740 | Prec: 0.6740 | Rec: 0.6740 | F1: 0.6734\n",
            "Val   | Acc: 0.6638 | Prec: 0.6038 | Rec: 0.6677 | F1: 0.6239\n",
            "No improvement. Patience: 2/5\n",
            "Epoch 9/15 | Batch 282/282 | Loss: 0.5414 | Acc: 0.8750 | Prec: 0.8750 | Rec: 0.9028 | F1: 0.8694\n",
            "\n",
            "Epoch 9 Summary\n",
            "Train | Loss: 0.8632 | Acc: 0.6756 | Prec: 0.6757 | Rec: 0.6756 | F1: 0.6751\n",
            "Val   | Acc: 0.6694 | Prec: 0.6109 | Rec: 0.6727 | F1: 0.6307\n",
            "✔ Model improved. Saved.\n",
            "Epoch 10/15 | Batch 282/282 | Loss: 1.1386 | Acc: 0.5000 | Prec: 0.5667 | Rec: 0.6167 | F1: 0.5270\n",
            "\n",
            "Epoch 10 Summary\n",
            "Train | Loss: 0.8524 | Acc: 0.6810 | Prec: 0.6815 | Rec: 0.6810 | F1: 0.6806\n",
            "Val   | Acc: 0.6668 | Prec: 0.6100 | Rec: 0.6730 | F1: 0.6302\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 11/15 | Batch 282/282 | Loss: 0.7295 | Acc: 0.7500 | Prec: 0.8333 | Rec: 0.7361 | F1: 0.7484\n",
            "\n",
            "Epoch 11 Summary\n",
            "Train | Loss: 0.8421 | Acc: 0.6806 | Prec: 0.6812 | Rec: 0.6806 | F1: 0.6804\n",
            "Val   | Acc: 0.6707 | Prec: 0.6159 | Rec: 0.6787 | F1: 0.6363\n",
            "✔ Model improved. Saved.\n",
            "Epoch 12/15 | Batch 282/282 | Loss: 0.7312 | Acc: 0.7500 | Prec: 0.6528 | Rec: 0.7063 | F1: 0.6418\n",
            "\n",
            "Epoch 12 Summary\n",
            "Train | Loss: 0.8328 | Acc: 0.6837 | Prec: 0.6848 | Rec: 0.6837 | F1: 0.6837\n",
            "Val   | Acc: 0.6781 | Prec: 0.6199 | Rec: 0.6768 | F1: 0.6389\n",
            "✔ Model improved. Saved.\n",
            "Epoch 13/15 | Batch 282/282 | Loss: 1.0174 | Acc: 0.6250 | Prec: 0.6250 | Rec: 0.5694 | F1: 0.5917\n",
            "\n",
            "Epoch 13 Summary\n",
            "Train | Loss: 0.8264 | Acc: 0.6896 | Prec: 0.6900 | Rec: 0.6896 | F1: 0.6892\n",
            "Val   | Acc: 0.6768 | Prec: 0.6186 | Rec: 0.6811 | F1: 0.6388\n",
            "No improvement. Patience: 1/5\n",
            "Epoch 14/15 | Batch 282/282 | Loss: 0.8506 | Acc: 0.8125 | Prec: 0.7778 | Rec: 0.8056 | F1: 0.7722\n",
            "\n",
            "Epoch 14 Summary\n",
            "Train | Loss: 0.8275 | Acc: 0.6883 | Prec: 0.6895 | Rec: 0.6883 | F1: 0.6882\n",
            "Val   | Acc: 0.6752 | Prec: 0.6182 | Rec: 0.6814 | F1: 0.6387\n",
            "No improvement. Patience: 2/5\n",
            "Epoch 15/15 | Batch 282/282 | Loss: 0.8688 | Acc: 0.6875 | Prec: 0.6667 | Rec: 0.7222 | F1: 0.6500\n",
            "\n",
            "Epoch 15 Summary\n",
            "Train | Loss: 0.8117 | Acc: 0.6957 | Prec: 0.6965 | Rec: 0.6957 | F1: 0.6954\n",
            "Val   | Acc: 0.6832 | Prec: 0.6249 | Rec: 0.6842 | F1: 0.6454\n",
            "✔ Model improved. Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i93Hq088-j08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}